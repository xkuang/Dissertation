{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import os\n",
    "import fileinput\n",
    "import plotly.graph_objs as go\n",
    "import sklearn\n",
    "import struct\n",
    "import re\n",
    "import csv\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from scipy.cluster.vq import whiten\n",
    "from sklearn import cluster\n",
    "from sklearn import manifold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import decomposition\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import urllib3\n",
    "from sklearn.cross_decomposition import CCA\n",
    "import plotly.plotly as py\n",
    "import http\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "import plotly.graph_objs as go\n",
    "from numpy import genfromtxt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the data from the Google cloud for FOLD_3. (ON THE GOOGLE CLOUD STORAGE)\n",
    "\n",
    "# Import the name of files from virtual machine\n",
    "\n",
    "my_data = genfromtxt(r'/home/uriot_thomas/FOLD3.txt', delimiter='\\n', dtype=str)\n",
    "\n",
    "url =  \"https://storage.googleapis.com/bucket-pinouche/ImperialData/FOLD_3\"\n",
    "\n",
    "dic = {}\n",
    "number_of_files = 0\n",
    "\n",
    "for filename in my_data:\n",
    "    print(filename)\n",
    "    f = urllib.request.urlopen(url+'/'+filename)\n",
    "    with open(os.path.basename(url), \"wb\") as local_file:\n",
    "        local_file.write(f.read())\n",
    "        dic[filename] = sio.loadmat(local_file.name)\n",
    "    \n",
    "    number_of_files = number_of_files + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Annotation folder is stored on the virtual machine instance directly (ON THE VIRTUAL MACHINE)\n",
    "\n",
    "even_names_files = list()\n",
    "index = 0\n",
    "\n",
    "for name in my_data:\n",
    "    if(index%2 == 0):\n",
    "        even_names_files.append(name[:-10])\n",
    "    index += 1\n",
    "\n",
    "dic2 = {}\n",
    "number_of_files = 0\n",
    "path = r'/home/uriot_thomas/two'\n",
    "\n",
    "for filename in even_names_files:\n",
    "    dic2[filename] = sio.loadmat(path+'/'+filename+'/'+'meanAnnotation.mat')\n",
    "    number_of_files = number_of_files + 1\n",
    "    \n",
    "sequences_shape = list()\n",
    "mean_annotations = list()\n",
    "\n",
    "for filename in even_names_files:\n",
    "    mean_annotations.append(dic2[filename]['annotations'])\n",
    "    sequences_shape.append(dic2[filename]['annotations'].shape[0])\n",
    "\n",
    "array_annotations_FOLD_3 = np.concatenate(np.asarray(mean_annotations), axis=0)\n",
    "\n",
    "print([number_of_files, array_annotations_FOLD_3.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the features data into usable matrix format\n",
    "\n",
    "def dataMatrix(array1, array2):\n",
    "\n",
    "    person1 = list()\n",
    "    person2 = list()\n",
    "\n",
    "    for frame in range(0, array1.shape[2]):\n",
    "        for sift in range(0,49):\n",
    "            person1.append(array1[sift,:,frame])\n",
    "            person2.append(array2[sift,:,frame])\n",
    "\n",
    "    tmp_arr_mat1 = np.reshape(np.asarray(person1),(array1.shape[2],6272))\n",
    "    tmp_arr_mat2 = np.reshape(np.asarray(person2),(array2.shape[2],6272))\n",
    "    frame_vector = np.concatenate([tmp_arr_mat1,tmp_arr_mat2], axis=1)\n",
    "    \n",
    "    return(frame_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Get all the values from the SIFT values for each of the frame from the dictionnary\n",
    "\n",
    "key=0\n",
    "concat = []\n",
    "\n",
    "for val in my_data:\n",
    "    if(key%2 == 0):\n",
    "        person1 = dic[val]['SIFT']\n",
    "    else:\n",
    "        person2 = dic[val]['SIFT']\n",
    "        concat.append(dataMatrix(person1, person2))\n",
    "    key +=1\n",
    "        \n",
    "Big_list = np.concatenate(concat)\n",
    "print(Big_list.shape)\n",
    "\n",
    "# The dimensions of the Big_list (data matrix) is 20640 by 12544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Instead of concatenating together the vectors for the 2 people, use the average for the data matrix. Note that later on,\n",
    "# the response we have for each frame is the same for the 2 people (i.e there is a conflict intensity for the overall \n",
    "# interaction and not a seperated conflict value for each of the interlocutors). So this approach may be a better way.\n",
    "\n",
    "Person1_columns = Big_list[:,:(49*128)]\n",
    "Person2_columns = Big_list[:,(49*128):]\n",
    "print([Person1_columns.shape,Person2_columns.shape])\n",
    "Average_Big_list_FOLD_3 = (Person1_columns + Person2_columns)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot of an example of the ground truth of conflict intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plots the annotation for a whole video sequence as an example\n",
    "\n",
    "mean_contents1 = array_annotations_FOLD_3[:len(dic['20120213_seq1_01_01.mat']['SIFT'][1][1])]\n",
    "\n",
    "\n",
    "range_array = np.asarray(list(range(0, len(dic['20120213_seq1_01_01.mat']['SIFT'][1][1]), 1)))\n",
    "\n",
    "fig = plt.figure(figsize=(17, 6))\n",
    "plt.plot(range_array , mean_contents1)\n",
    "plt.axis([0,len(dic['20120213_seq1_01_01.mat']['SIFT'][1][1]), 0, 1])\n",
    "fig.suptitle('Ground truth conflict intensity between the 2 participants')\n",
    "plt.xlabel('Number of frames')\n",
    "plt.ylabel('Conflict intensity')\n",
    "plt.show()\n",
    "fig.savefig('meanplot.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PCA on the new matrix\n",
    "\n",
    "n_components = 1200\n",
    "svd = decomposition.TruncatedSVD(n_components=n_components, algorithm='arpack')\n",
    "svd.fit(Average_Big_list_FOLD_3)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "\n",
    "# 500 components: 81%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scree plot\n",
    "\n",
    "range_array = np.asarray(list(range(0, svd.explained_variance_ratio_.shape[0], 1)))\n",
    "\n",
    "fig = plt.figure(figsize=(17, 6))\n",
    "plt.plot(range_array , svd.explained_variance_ratio_)\n",
    "plt.axis([0,svd.explained_variance_ratio_.shape[0], 0, 0.1])\n",
    "fig.suptitle('Scree plot')\n",
    "plt.xlabel('Components')\n",
    "plt.ylabel('Percentage of variance explained')\n",
    "plt.show()\n",
    "fig.savefig('scree.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the new data of dimension  (number of frames * 500)\n",
    "\n",
    "svd_data = np.dot(Average_Big_list_FOLD_3,np.transpose(svd.components_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save a dictionary into a pickle file.\n",
    "import pickle\n",
    "\n",
    "pickle.dump(svd_data, open( \"PCA_data.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dictionary back from the pickle file.\n",
    "\n",
    "svd_data = pickle.load( open( \"PCA_data.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using t-sne on the PCA data to visualize the data (only on 10000 data points for speed purposes)\n",
    "\n",
    "random_indices = np.random.choice(svd_data.shape[0], 10000, replace=False)\n",
    "sampled_rows = svd_data[random_indices, :]\n",
    "sampled_annotations = array_annotations_FOLD_3[random_indices, :]\n",
    "tsne_svd = manifold.TSNE(n_components=2, verbose=0, perplexity=30, n_iter=2500) \n",
    "tsne_results_svd = tsne_svd.fit_transform(sampled_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly \n",
    "plotly.tools.set_credentials_file(username='pinouche', api_key='lNAc8TgFyighg1amN5jI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kmeans = cluster.KMeans(n_clusters=3) # k-means\n",
    "kmeans.fit(sampled_rows)\n",
    "\n",
    "c = kmeans.labels_\n",
    "x = tsne_results_svd[:,0]\n",
    "y = tsne_results_svd[:,1]\n",
    "#z = tsne_results_svd[:,2]\n",
    "t = [str(s[0]) for s in sampled_annotations]\n",
    "\n",
    "trace1 = go.Scatter(x=x,y=y,text=t, mode='markers',marker=dict(size=12,color=c, colorscale = 'Viridis', opacity=0.8))\n",
    "data = [trace1]\n",
    "layout = go.Layout(margin=dict(l=0,r=0,b=0,t=0))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='t-SNE PCA with k-Means')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the data with an RBF Gaussian kernal and perform PCA. Could again use k-means and plot the data in 2 or 3 dimensions\n",
    "# using t-SNE as was done for PCA above.\n",
    "\n",
    "kpca = KernelPCA(kernel=\"cosine\")\n",
    "X = kpca.fit_transform(Average_Big_list_FOLD_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# see how much variance the first 500 components are explaining\n",
    "\n",
    "print(np.sum(kpca.lambdas_[:1000])/np.sum(kpca.lambdas_))\n",
    "Kernel_data = X[:,:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the cubic kernel pca with parameter = 1/number of features\n",
    "\n",
    "pickle.dump(Kernel_data, open( \"KPCA_data.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the kernel cubic with parameter = 1/number of features\n",
    "\n",
    "Kernel_cubic = pickle.load( open( \"KPCA_data.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Canonical correlation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading the name of files of FOLD_4\n",
    "\n",
    "my_data_Fold_4 = genfromtxt(r'/home/uriot_thomas/FOLD4.txt', delimiter='\\n', dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load the data from the Google cloud for FOLD_4. \n",
    "# Note that the variable name_of_files is the same one we define in a couple of cells below. (ON THE GOOGLE CLOUD STORAGE)\n",
    "\n",
    "url =  \"https://storage.googleapis.com/bucket-pinouche/ImperialData/FOLD_4\"\n",
    "\n",
    "dic = {}\n",
    "number_of_files = 0\n",
    "\n",
    "for filename in my_data_Fold_4 :\n",
    "    print(filename)\n",
    "    f = urllib.request.urlopen(url+'/'+filename)\n",
    "    with open(os.path.basename(url), \"wb\") as local_file:\n",
    "        local_file.write(f.read())\n",
    "        dic[filename] = sio.loadmat(local_file.name)\n",
    "    \n",
    "    number_of_files = number_of_files + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create a similar matrix to Average_big_list but using the data FOLD_4. This is a sueprvised methods (it uses the labels) and\n",
    "# we do not want to use the labels to transform our data which will then be trained using the same labels (introduces bias).\n",
    "\n",
    "# Get all the values from the SIFT values for each of the frame from the dictionnary\n",
    "\n",
    "key=0\n",
    "concat = []\n",
    "\n",
    "for val in my_data_Fold_4:\n",
    "    if(key%2 == 0):\n",
    "        person1 = dic[val]['SIFT']\n",
    "    else:\n",
    "        person2 = dic[val]['SIFT']\n",
    "        concat.append(dataMatrix(person1, person2))\n",
    "    key +=1\n",
    "        \n",
    "Big_list = np.concatenate(concat)\n",
    "print(Big_list.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the average for the two people\n",
    "\n",
    "Person1_columns = Big_list[:,:(49*128)]\n",
    "Person2_columns = Big_list[:,(49*128):]\n",
    "print([Person1_columns.shape,Person2_columns.shape])\n",
    "Average_Big_list_FOLD_4 = (Person1_columns + Person2_columns)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Annotation folder is stored on the virtual machine instance directly (ON THE VIRTUAL MACHINE) for FOLD_4\n",
    "\n",
    "even_names_files = list()\n",
    "index = 0\n",
    "\n",
    "for name in my_data_Fold_4:\n",
    "    if(index%2 == 0):\n",
    "        even_names_files.append(name[:-10])\n",
    "    index += 1\n",
    "\n",
    "dic2 = {}\n",
    "number_of_files = 0\n",
    "path = r'/home/uriot_thomas/two'\n",
    "\n",
    "for filename in even_names_files:\n",
    "    dic2[filename] = sio.loadmat(path+'/'+filename+'/'+'meanAnnotation.mat')\n",
    "    number_of_files = number_of_files + 1\n",
    "    \n",
    "sequences_shape = list()\n",
    "mean_annotations = list()\n",
    "\n",
    "for filename in even_names_files:\n",
    "    mean_annotations.append(dic2[filename]['annotations'])\n",
    "    sequences_shape.append(dic2[filename]['annotations'].shape[0])\n",
    "\n",
    "array_annotations_FOLD_4 = np.concatenate(np.asarray(mean_annotations), axis=0)\n",
    "\n",
    "print([number_of_files, array_annotations_FOLD_4.shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perform CCA\n",
    "\n",
    "new = np.repeat(array_annotations_FOLD_4,Average_Big_list_FOLD_4.shape[1] , axis=1)\n",
    "print(new[2000,:],array_annotations_FOLD_4[2000])\n",
    "cca = CCA(n_components=2, scale=True, max_iter=500, tol=1e-06, copy=True)\n",
    "cca.fit(Average_Big_list_FOLD_4, array_annotations_FOLD_4)  # THIS IS THE TRAINING PART WHICH NEEDS TO BE DONE ON A DIFFERENT FOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "\n",
    "X_c, Y_c = cca.transform(Average_Big_list_FOLD_3, array_annotations_FOLD_3) # HERE THIS IS THE USUAL DATA ON FOLD_3\n",
    "print(X_c[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32 # Take it as mutiple of the number of rows of the training data to use all of it\n",
    "original_dim = 1200 # number of variables in the training data\n",
    "latent_dim = 30\n",
    "intermediate_dim = 600\n",
    "epochs = 20\n",
    "epsilon_std = 1.0 # Standard deviation of epsilon to be used the reparametrization trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the encoder network\n",
    "\n",
    "x = Input(batch_shape=(batch_size, original_dim))\n",
    "h = Dense(intermediate_dim, activation='relu')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_sigma = Dense(latent_dim)(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining a sampling function to sample the latent value z using the reparametrization trick\n",
    "\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_dim),mean=0., stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_sigma) * epsilon\n",
    "\n",
    "# note that \"output_shape\" isn't necessary with the TensorFlow backend\n",
    "# so you could write `Lambda(sampling)([z_mean, z_log_sigma])`\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the decoder network\n",
    "\n",
    "decoder_h = Dense(intermediate_dim, activation='relu')\n",
    "decoder_mean = Dense(original_dim, activation='sigmoid')\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# end-to-end autoencoder\n",
    "vae = Model(x, x_decoded_mean)\n",
    "\n",
    "# encoder, from inputs to latent space\n",
    "encoder = Model(x, z_mean)\n",
    "\n",
    "# generator, from latent space to reconstructed inputs\n",
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "generator = Model(decoder_input, _x_decoded_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining the loss function with a normal suboptimal variational factor and a normal likelihood where the parameters of\n",
    "# the normals are defined by the encoder and decoder networks\n",
    "\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = metrics.binary_crossentropy(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma), axis=-1)\n",
    "    return xent_loss + kl_loss\n",
    "\n",
    "vae.compile(optimizer='rmsprop', loss=vae_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that makes a train and validations sets out of the input data\n",
    "\n",
    "def MakeData(data,labels):\n",
    "    validation_set_size = round(data.shape[0]/5)\n",
    "    random_indices_val = np.random.choice(data.shape[0], validation_set_size, replace=False)\n",
    "    \n",
    "    val_set = data[random_indices_val, :]\n",
    "    train_set = np.delete(data, random_indices_val, 0)\n",
    "    val_labels = labels[random_indices_val, :]\n",
    "    train_labels = np.delete(labels, random_indices_val, 0)\n",
    "\n",
    "    \n",
    "    return val_set , train_set, val_labels, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the VAE for the svd_data PCA data. Could do a loop and take the average of the validation losses to get more consistant\n",
    "# values for the validation loss but this does not matter for very large data set and it is also computationally very expensive.\n",
    "\n",
    "x_val, x_train, y_val, y_train = MakeData(svd_data,array_annotations_FOLD_3)\n",
    "print([x_train.shape,x_val.shape,y_val.shape,y_train.shape])\n",
    "variational = vae.fit(x_train, x_train,epochs=10,batch_size=batch_size,validation_data=(x_val, x_val), verbose=0,initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print([variational.history.keys(),vae.evaluate(x_val,x_val,verbose=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Obtain the validation_loss\n",
    "\n",
    "fig = plt.figure(figsize=(17, 6))\n",
    "plt.plot(variational.history['loss'])\n",
    "plt.plot(variational.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "fig.savefig('newfiguseless.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit all the data\n",
    "\n",
    "vae.fit(svd_data, svd_data,epochs=35,batch_size=batch_size,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the latent once the model has been trained\n",
    "\n",
    "VAE_data = encoder.predict(svd_data, batch_size=batch_size)\n",
    "print(VAE_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the VAE applied on the svd_data 100 latents and 600 intermediate dimensions for the hidden layer\n",
    "\n",
    "pickle.dump(VAE_data, open( \"VAE_SVD_100_650.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the data\n",
    "\n",
    "VAE_SVD_100_650 = pickle.load( open( \"VAE_SVD_100_650.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import resample\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "#from hyperopt import STATUS_OK, hp, fmin, tpe, Trials, space_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pick indices and always keep the same to be used in the function below\n",
    "\n",
    "validation_set_size = round(svd_data.shape[0]/5)\n",
    "random_indices_val = np.random.choice(svd_data.shape[0], validation_set_size, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the random indices to us e the same one as validation \n",
    "\n",
    "pickle.dump(random_indices_val, open( \"Random_indices_MLP.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Retrieve the data\n",
    "\n",
    "random_indices_val = pickle.load( open( \"Random_indices_MLP.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a validation and training set using the indices defined above\n",
    "    \n",
    "def MakeData(data,labels,indices):\n",
    "    val_set = data[indices, :]\n",
    "    train_set = np.delete(data, indices, 0)\n",
    "    val_labels = labels[indices, :]\n",
    "    train_labels = np.delete(labels, indices, 0)\n",
    "\n",
    "    return val_set , train_set, val_labels, train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multi layer perceptron to predict the conflict intensity labels. Need to taylor it for the number of hidden layers according\n",
    "# to what the input data is.\n",
    "\n",
    "def MLP_Layers(predictors,labels,random_indices_val,number_of_neurons1):\n",
    "    model_dnn = MLPRegressor(hidden_layer_sizes=(number_of_neurons1,),max_iter=20,early_stopping=True)\n",
    "    x_val, x_train, y_val, y_train = MakeData(predictors,labels,random_indices_val)\n",
    "    model_dnn.fit(x_train, y_train)\n",
    "    predictions = model_dnn.predict(x_val)\n",
    "    return(mean_squared_error(predictions, y_val))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run the MLP function and return the MSE to choose the best model and tune the number of neurons and layers\n",
    "\n",
    "MLP_Layers(VAE_SVD_100_650, array_annotations_FOLD_3, random_indices_val, 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the best model onthe svd_data (data just with PCA)\n",
    "\n",
    "\n",
    "MLP_Layers(svd_data, array_annotations_FOLD_3, random_indices_val, 70)\n",
    "\n",
    "#model_dnn = MLPRegressor(hidden_layer_sizes=(450,225,),max_iter=3000,early_stopping=True)\n",
    "#model_dnn.fit(svd_data, array_annotations_FOLD_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the best model on the PCA + VAE data with 30 latents\n",
    "\n",
    "MLP_Layers(VAE_SVD_30_600, array_annotations_FOLD_3, random_indices_val, 70)\n",
    "\n",
    "#model_dnn_VAE30 = MLPRegressor(hidden_layer_sizes=(20,),max_iter=3000,early_stopping=True)\n",
    "#model_dnn_VAE30.fit(VAE_SVD_30_600, array_annotations_FOLD_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that puts 19 video sequences in training set and keep the 20th as validation\n",
    "\n",
    "def Makelist(data,sequences_shape=sequences_shape):\n",
    "    list_data = []\n",
    "    for index in range(0,len(sequences_shape)):\n",
    "        list_data.append(data[sequences_shape[index]:(sequences_shape[index]+sequences_shape[index]),:])\n",
    "    return(list_data)\n",
    "        \n",
    "        \n",
    "def Dotraining(Mylist):\n",
    "    train_data = []\n",
    "    val_data = []\n",
    "    print(len(Mylist))\n",
    "    indices = np.array([x for x in range(len(Mylist))])\n",
    "    \n",
    "    for index in range(0,len(Mylist)):\n",
    "        l = Mylist\n",
    "        val_data.append(l[index])\n",
    "        trainn = [x for i,x in enumerate(Mylist) if i!=index]\n",
    "        train_data.append(trainn)\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert the list of list into a usable numpy array to feed into the function split2sequences\n",
    "\n",
    "l = Makelist(array_annotations_FOLD_3,sequences_shape)\n",
    "train,val = Dotraining(l)\n",
    "\n",
    "flat_list = list()\n",
    "for sublist in train:\n",
    "    for item in sublist:\n",
    "        flat_list.append(item)\n",
    "flat_array = np.array(flat_list)\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for index in range(0,len(l)):\n",
    "    data_list.append(np.concatenate(flat_array[(index*(len(l)-1)):(len(l)-1)*index+(len(l)-1),], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the function to split the inputs and outputs of the LSTM\n",
    "input_length = 10\n",
    "\n",
    "def split2sequencesTrain(data, length_x=input_length, length_y=1, split=1):\n",
    "    step = 1\n",
    "    xN = []\n",
    "    yN = []\n",
    "    \n",
    "    for i in range(0, len(data) - length_x , step):\n",
    "        xN.append(data[i: i + length_x])\n",
    "        yN.append(data[i+1+length_x-length_y:i + length_x + 1])\n",
    "        \n",
    "    train_size = int(len(xN) * split)\n",
    "    test_size = len(xN) - train_size\n",
    "    \n",
    "    xN = np.array(xN)\n",
    "    yN = np.array(yN)\n",
    "    n = len(data)\n",
    "    X_train, X_test = xN[0:train_size],  xN[train_size:n]\n",
    "    Y_train,Y_test = yN[0:train_size], yN[train_size:n]\n",
    "\n",
    "    return xN, yN, X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def split2sequencesValidation(data, length_x=input_length, length_y=1, split=0):\n",
    "    step = 1\n",
    "    xN = []\n",
    "    yN = []\n",
    "    \n",
    "    for i in range(0, len(data) - length_x, step):\n",
    "        xN.append(data[i: i + length_x])\n",
    "        yN.append(data[i+1+length_x-length_y:i + length_x + 1])\n",
    "        \n",
    "    train_size = int(len(xN) * split)\n",
    "    test_size = len(xN) - train_size\n",
    "    \n",
    "    xN = np.array(xN)\n",
    "    yN = np.array(yN)\n",
    "    n = len(data)\n",
    "    X_train, X_test = xN[0:train_size],  xN[train_size:n]\n",
    "    Y_train,Y_test = yN[0:train_size], yN[train_size:n]\n",
    "\n",
    "    return xN, yN, X_train, X_test, Y_train, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the model here\n",
    "\n",
    "N_HIDDEN = 32\n",
    "N_HIDDEN2 = 16\n",
    "N_DENSE = 1 # number of outputs (1 for label array)\n",
    "epochs = 100\n",
    "batch_size = 16\n",
    "input_dim = 1\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(N_HIDDEN, input_shape = (input_length,input_dim)))\n",
    "#model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='relu'))\n",
    "model.compile(loss='mse', optimizer=RMSprop(lr=0.000001, rho=0.9, epsilon=1e-08), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function that takes as input the video sequence to be taken as validation. The 19 remaining video sequences are then\n",
    "# automatically pointed to and fed to the LSTM sequence by sequence such that the LSTM input does not have any overlap \n",
    "# (i.e, when there is a transition from 2 video sequences, there is no time depency and so we do not want our LSTM to learn from\n",
    "# these erroneous inputs).\n",
    "\n",
    "\n",
    "model.save('my_lstm.h5')\n",
    "\n",
    "def TrainLSTM1(fold_number):\n",
    "    \n",
    "    for count in range(0,epochs):\n",
    "        list_val = list()\n",
    "        valid = val[fold_number]\n",
    "        xN, yN, X_train, X_test1, Y_train, Y_test1 = split2sequencesValidation(valid)\n",
    "        Y_test1 = np.reshape(Y_test1, (Y_test1.shape[0], Y_test1.shape[1]))\n",
    "        #print([Y_test1.shape,X_test1.shape])\n",
    "        for index2 in range(0,number_of_files-1):\n",
    "        \n",
    "            model = load_model('my_lstm.h5')\n",
    "            xN, yN, X_train, X_test, Y_train, Y_test = split2sequencesTrain(flat_array[index2+(number_of_files-1)*fold_number])\n",
    "            Y_train = np.reshape(Y_train, (Y_train.shape[0], Y_train.shape[1]))\n",
    "            model.fit(X_train, Y_train, batch_size=32, epochs=1, verbose=0, validation_data=None)\n",
    "            model.save('my_lstm.h5')\n",
    "        \n",
    "           \n",
    "        new_loss = model.evaluate(X_test1, Y_test1,verbose=0)\n",
    "        list_val.append(new_loss)\n",
    "    #model = load_model('my_lstm.h5')\n",
    "    return(list_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Call the function with the first video sequence as validation set and the the19 remaining sequences as training\n",
    "\n",
    "val_values = TrainLSTM1(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The above function works but saving and loading the model to the hardware takes to much time. Instead we first apply the\n",
    "# Split function to the 19 video sequences separately and then concatenate them to directly input into the model.fit\n",
    "\n",
    "def SplitVidByVid(fold_number):\n",
    "    \n",
    "    my_list_Y_train = []\n",
    "    my_list_X_train = []\n",
    "    valid = val[fold_number]\n",
    "    xN, yN, X_train, X_test1, Y_train, Y_test1 = split2sequencesValidation(valid)\n",
    "    Y_test1 = np.reshape(Y_test1, (Y_test1.shape[0], Y_test1.shape[1]))\n",
    "        \n",
    "    for index2 in range(0,number_of_files-1):\n",
    "        \n",
    "        xN, yN, X_train, X_test, Y_train, Y_test = split2sequencesTrain(flat_array[index2+(number_of_files-1)*fold_number])\n",
    "        Y_train = np.reshape(Y_train, (Y_train.shape[0], Y_train.shape[1]))\n",
    "        my_list_Y_train.append(Y_train)\n",
    "        my_list_X_train.append(X_train)\n",
    "        \n",
    "    Full_splitted_data_Y = np.concatenate(my_list_Y_train)\n",
    "    Full_splitted_data_X = np.concatenate(my_list_X_train)\n",
    "    return X_test1, Y_test1, Full_splitted_data_X, Full_splitted_data_Y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, Y_test, X_train, Y_train = SplitVidByVid(0)\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"Y_train shape: \", Y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"Y_test shape: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "red = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=0, mode='auto', epsilon=0.00001, cooldown=0, min_lr=0.00000001)\n",
    "monitor = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=0, mode='auto')\n",
    "lstm = model.fit(X_train, Y_train, batch_size=32, epochs=100, verbose=0, validation_data=(X_test,Y_test), callbacks=[monitor,red])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print([lstm.history['loss'][98],lstm.history['val_loss'][98]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the validation and training loss side to side\n",
    "\n",
    "fig = plt.figure(figsize=(17, 6))\n",
    "plt.plot(lstm.history['loss'])\n",
    "plt.plot(lstm.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "fig.savefig('newfiguseless.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the validation and training loss from the 30th epoch onwards on an appropriately scaled y axis\n",
    "\n",
    "fig = plt.figure(figsize=(17, 6))\n",
    "plt.plot(lstm.history['loss'][30:])\n",
    "plt.plot(lstm.history['val_loss'][30:])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "fig.savefig('newfiguseless.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict the intensity conflict labels for the chosen video sequence\n",
    "\n",
    "predicted_values = model.predict(X_test, verbose=0)\n",
    "print([predicted_values.shape, val[0][(input_length):,].shape])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the true intensity labels with the one predicted from the LSTM\n",
    "\n",
    "fig = plt.figure(figsize=(17, 6))\n",
    "plt.plot(predicted_values)\n",
    "plt.plot(val[0][(input_length):,])\n",
    "plt.title('Model prediction vs real labels')\n",
    "plt.ylabel('Conflict intensity')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend(['Predicted labels', 'True labels'], loc='upper left')\n",
    "plt.show()\n",
    "fig.savefig('newfiguseless.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function that use the first lstm predicted value as an input value in next lstm prediction\n",
    "\n",
    "from numpy import zeros, newaxis\n",
    "\n",
    "def AbsolutePredict(test):\n",
    "    predicted_val = list()\n",
    "    \n",
    "    for index in range(0,(test.shape[0]-1)):\n",
    "    \n",
    "        predicted_val.append(model.predict(test[index,:][newaxis,...], verbose=0))\n",
    "        test[index+1,(input_length-1)] = predicted_val[index]\n",
    "        \n",
    "    return(predicted_val)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AbsPred_array2 = np.asarray(AbsolutePredict(X_test))\n",
    "AbsPred_array2 = np.reshape(AbsPred_array2, (AbsPred_array2.shape[0], AbsPred_array2.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(17, 6))\n",
    "plt.plot(AbsPred_array2)\n",
    "plt.plot(val[0][(input_length):(val[0].shape[0]-1),])\n",
    "plt.plot(predicted_values[:(val[0].shape[0]-1),])\n",
    "plt.title('Model prediction vs real labels')\n",
    "plt.ylabel('Conflict intensity')\n",
    "plt.xlabel('Epochs')\n",
    "plt.legend([' Absolute prediction','True labels', 'Predicted values'], loc='upper left')\n",
    "plt.show()\n",
    "fig.savefig('newfiguseless.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
